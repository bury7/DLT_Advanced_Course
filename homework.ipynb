{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P0X22pjILSY"
      },
      "source": [
        "# **Homework: Speed up your pipeline**\n",
        "\n",
        "### **Goal**\n",
        "\n",
        "Use the public **Jaffle Shop API** to build a `dlt` pipeline and apply everything you've learned about performance:\n",
        "\n",
        "- Chunking\n",
        "- Parallelism\n",
        "- Buffer control\n",
        "- File rotation\n",
        "- Worker tuning\n",
        "\n",
        "Your task is to **make the pipeline as fast as possible**, while keeping the results correct.\n",
        "\n",
        "### **What you’ll need**\n",
        "\n",
        "- API base: `https://jaffle-shop.scalevector.ai/api/v1`\n",
        "- Docs: [https://jaffle-shop.scalevector.ai/docs](https://jaffle-shop.scalevector.ai/docs)\n",
        "- Start with these endpoints:\n",
        "  - `/customers`\n",
        "  - `/orders`\n",
        "  - `/products`\n",
        "\n",
        "Each of them returns **paged responses** — so you'll need to handle pagination.\n",
        "\n",
        "### **What to implement**\n",
        "\n",
        "1. **Extract** from the API using `dlt`\n",
        "\n",
        "   - Use `dlt.resource` and [`RESTClient`](https://dlthub.com/docs/devel/general-usage/http/rest-client) with proper pagination\n",
        "\n",
        "2. **Apply all performance techniques**\n",
        "\n",
        "   - Group resources into sources\n",
        "   - Yield **chunks/pages**, not single rows\n",
        "   - Use `parallelized=True`\n",
        "   - Set `EXTRACT__WORKERS`, `NORMALIZE__WORKERS`, and `LOAD__WORKERS`\n",
        "   - Tune buffer sizes and enable **file rotation**\n",
        "\n",
        "3. **Measure performance**\n",
        "   - Time the extract, normalize, and load stages separately\n",
        "   - Compare a naive version vs. optimized version\n",
        "   - Log thread info or `pipeline.last_trace` if helpful\n",
        "\n",
        "### **Deliverables**\n",
        "\n",
        "Share your code as a Google Colab or [GitHub Gist](https://gist.github.com/) in Homework Google Form. **This step is required for certification.**\n",
        "\n",
        "It should include:\n",
        "\n",
        "- Working pipeline for at least 2 endpoints\n",
        "- Before/after timing comparison\n",
        "- A short explanation of what changes made the biggest difference if there're any differences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y95Ik5KI5kq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install dlt[duckdb]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwKeG28SNreB"
      },
      "source": [
        "## **Naive Version**\n",
        "\n",
        "In this first version, the pipeline extracts data from three endpoints:\n",
        "\n",
        "- `/customers`\n",
        "- `/orders`\n",
        "- `/products`\n",
        "\n",
        "This implementation:\n",
        "\n",
        "- Uses a small page size (`100`) for pagination.\n",
        "- Processes every row individually.\n",
        "- Runs sequentially without parallelization.\n",
        "- Uses default buffer and worker settings.\n",
        "\n",
        "This serves as a baseline to compare the impact of later optimizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1kTiC-nI-BN",
        "outputId": "09663a22-179c-4516-b5be-21b7d2154aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run started at 2025-07-30 21:03:42.431935+00:00 and COMPLETED in 10 minutes and 41.80 seconds with 4 steps.\n",
            "Step extract COMPLETED in 10 minutes and 10.22 seconds.\n",
            "\n",
            "Load package 1753909422.4734976 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 13.39 seconds.\n",
            "Normalized data for the following tables:\n",
            "- _dlt_pipeline_state: 1 row(s)\n",
            "- products: 10 row(s)\n",
            "- orders: 61948 row(s)\n",
            "- orders__items: 90900 row(s)\n",
            "- customers: 935 row(s)\n",
            "\n",
            "Load package 1753909422.4734976 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 18.16 seconds.\n",
            "Pipeline jaffle_shop_pipeline load step completed in 17.23 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250730090342\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline.duckdb location to store data\n",
            "Load package 1753909422.4734976 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 10 minutes and 41.80 seconds.\n",
            "Pipeline jaffle_shop_pipeline load step completed in 17.23 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250730090342\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline.duckdb location to store data\n",
            "Load package 1753909422.4734976 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "from collections.abc import Iterator\n",
        "from typing import Any\n",
        "\n",
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import PageData, RESTClient\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
        "\n",
        "\n",
        "@dlt.resource(name=\"customers\")\n",
        "def get_customers() -> Iterator[PageData[Any]]:\n",
        "    client = RESTClient(\n",
        "        base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "        paginator=HeaderLinkPaginator(),\n",
        "    )\n",
        "    params = {\"page_size\": 100}\n",
        "    for page in client.paginate(\"/customers\", params=params):\n",
        "        yield from page\n",
        "\n",
        "\n",
        "@dlt.resource(name=\"orders\")\n",
        "def get_orders() -> Iterator[PageData[Any]]:\n",
        "    client = RESTClient(\n",
        "        base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "        paginator=HeaderLinkPaginator(),\n",
        "    )\n",
        "    params = {\"page_size\": 100}\n",
        "    for page in client.paginate(\"/orders\", params=params):\n",
        "        yield from page\n",
        "\n",
        "\n",
        "@dlt.resource(name=\"products\")\n",
        "def get_products() -> Iterator[PageData[Any]]:\n",
        "    client = RESTClient(\n",
        "        base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "        paginator=HeaderLinkPaginator(),\n",
        "    )\n",
        "    params = {\"page_size\": 100}\n",
        "    for page in client.paginate(\"/products\", params=params):\n",
        "        yield from page\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"jaffle_shop_pipeline\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"jaffle_shop\",\n",
        "    dev_mode=True,\n",
        ")\n",
        "\n",
        "load_info = pipeline.run([get_customers, get_orders, get_products])\n",
        "print(pipeline.last_trace)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn-2GPxDNvu1"
      },
      "source": [
        "## **Optimized Version**\n",
        "\n",
        "In this improved version, several optimizations are applied to speed up the pipeline:\n",
        "\n",
        "1. **Chunking:**\n",
        "\n",
        "   - Increased page size from 100 → 5000 to reduce the number of API calls and the overhead of small requests.\n",
        "   - Instead of yielding individual rows, pages are yielded in bulk for faster processing.\n",
        "\n",
        "2. **Parallelism:**\n",
        "\n",
        "   - Added `parallelized=True` to resources, allowing concurrent data extraction from multiple endpoints.\n",
        "\n",
        "3. **Worker tuning:**\n",
        "\n",
        "   - Increased the number of workers for extract, normalize, and load stages to make full use of available CPU resources.\n",
        "\n",
        "4. **Buffer control and file rotation:**\n",
        "\n",
        "   - Increased buffer sizes and allowed file rotation to reduce memory overhead and improve write speeds.\n",
        "\n",
        "5. **Grouping resources into a single source:**\n",
        "   - Resources were grouped in a `@dlt.source` to keep the pipeline structure cleaner and run all related endpoints together.\n",
        "\n",
        "These changes aim to make the extraction phase, which was the bottleneck in the naive version, much faster while keeping normalization and load times efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moGAsBKSI_W5",
        "outputId": "3a23f039-d356-47c7-e9a1-e9427336c1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run started at 2025-07-30 21:31:04.998949+00:00 and COMPLETED in 2 minutes and 33.04 seconds with 4 steps.\n",
            "Step extract COMPLETED in 2 minutes and 6.50 seconds.\n",
            "\n",
            "Load package 1753911065.024928 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 12.19 seconds.\n",
            "Normalized data for the following tables:\n",
            "- _dlt_pipeline_state: 1 row(s)\n",
            "- customers: 935 row(s)\n",
            "- products: 10 row(s)\n",
            "- orders: 61948 row(s)\n",
            "- orders__items: 90900 row(s)\n",
            "\n",
            "Load package 1753911065.024928 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 14.33 seconds.\n",
            "Pipeline jaffle_shop_pipeline_optimized load step completed in 14.29 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250730093104\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_optimized.duckdb location to store data\n",
            "Load package 1753911065.024928 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 2 minutes and 33.04 seconds.\n",
            "Pipeline jaffle_shop_pipeline_optimized load step completed in 14.29 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset jaffle_shop_20250730093104\n",
            "The duckdb destination used duckdb:////content/jaffle_shop_pipeline_optimized.duckdb location to store data\n",
            "Load package 1753911065.024928 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections.abc import Iterator\n",
        "from typing import Any\n",
        "\n",
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import PageData, RESTClient  # noqa: F811\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator  # noqa: F811\n",
        "\n",
        "os.environ[\"EXTRACT__WORKERS\"] = \"6\"\n",
        "os.environ[\"DATA_WRITER__BUFFER_MAX_ITEMS\"] = \"10000\"\n",
        "os.environ[\"EXTRACT__DATA_WRITER__FILE_MAX_ITEMS\"] = \"20000\"\n",
        "\n",
        "os.environ[\"NORMALIZE__WORKERS\"] = \"4\"\n",
        "os.environ[\"NORMALIZE__DATA_WRITER__BUFFER_MAX_ITEMS\"] = \"20000\"\n",
        "os.environ[\"NORMALIZE__DATA_WRITER__FILE_MAX_ITEMS\"] = \"20000\"\n",
        "\n",
        "os.environ[\"LOAD__WORKERS\"] = \"4\"\n",
        "\n",
        "\n",
        "@dlt.source(name=\"jaffle_shop_source\")\n",
        "def jaffle_shop_source() -> tuple[Any, Any, Any]:\n",
        "    @dlt.resource(name=\"customers\", parallelized=True)\n",
        "    def get_customers() -> Iterator[PageData[Any]]:\n",
        "        client = RESTClient(\n",
        "            base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "            paginator=HeaderLinkPaginator(),\n",
        "        )\n",
        "        params = {\"page_size\": 5000}\n",
        "        yield from client.paginate(\"/customers\", params=params)\n",
        "\n",
        "    @dlt.resource(name=\"orders\", parallelized=True)\n",
        "    def get_orders() -> Iterator[PageData[Any]]:\n",
        "        client = RESTClient(\n",
        "            base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "            paginator=HeaderLinkPaginator(),\n",
        "        )\n",
        "        params = {\"page_size\": 5000}\n",
        "        yield from client.paginate(\"/orders\", params=params)\n",
        "\n",
        "    @dlt.resource(name=\"products\", parallelized=True)\n",
        "    def get_products() -> Iterator[PageData[Any]]:\n",
        "        client = RESTClient(\n",
        "            base_url=\"https://jaffle-shop.scalevector.ai/api/v1\",\n",
        "            paginator=HeaderLinkPaginator(),\n",
        "        )\n",
        "        params = {\"page_size\": 5000}\n",
        "        yield from client.paginate(\"/products\", params=params)\n",
        "\n",
        "    return get_customers, get_orders, get_products\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"jaffle_shop_pipeline_optimized\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"jaffle_shop\",\n",
        "    dev_mode=True,\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(jaffle_shop_source())\n",
        "print(pipeline.last_trace)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Wa8SsbN2OU"
      },
      "source": [
        "## **Timing Comparison and Analysis**\n",
        "\n",
        "| Version   | Total Time | Extract  | Normalize | Load    |\n",
        "| --------- | ---------- | -------- | --------- | ------- |\n",
        "| Naive     | 10:41.80   | 10:10.22 | 13.39 s   | 18.16 s |\n",
        "| Optimized | 02:33.04   | 02:06.50 | 12.19 s   | 14.33 s |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TeupvvbPQVv"
      },
      "source": [
        "### **Observations**\n",
        "\n",
        "- The naive pipeline spent most of its time in the **extract step**, as small page sizes resulted in many sequential API calls.\n",
        "- The optimized pipeline:\n",
        "  - Used **larger chunks of data (5000 rows)**.\n",
        "  - Ran API requests in **parallel** with multiple workers.\n",
        "  - Increased buffer sizes and allowed file rotation to speed up I/O.\n",
        "  - Grouped resources into a `@dlt.source` for a cleaner, more structured pipeline definition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i73TIwiLPTr2"
      },
      "source": [
        "## **Result**\n",
        "\n",
        "The optimized pipeline completed about **4 times faster** than the naive version while maintaining data correctness. The biggest performance improvement came from **parallel extraction and larger chunk sizes**, reducing the overhead of multiple small API requests.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
